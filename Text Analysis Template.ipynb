{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4681c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586fb1a",
   "metadata": {},
   "source": [
    "# Импорт данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46c18dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c64c87",
   "metadata": {},
   "source": [
    "# Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3f3e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "f = io.open(r'stopwords-ru.txt','r',encoding='utf8')\n",
    "sw = f.read()\n",
    "sw = sw.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0092e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    temp = str(tweet)\n",
    "    temp = temp.lower()\n",
    "    temp = re.sub(\"#[A-Za-zА-Яа-я0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"\\n\",\" \", temp)\n",
    "    temp =  re.sub('[^а-яё ]', '', temp, flags=re.IGNORECASE)\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in sw]\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5946dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def lemmatize(doc):\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokens.append(token)\n",
    "    tokens = \" \".join(word for word in tokens if word not in sw)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81951dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemma'] = df['Текст'].apply(clean_tweet)\n",
    "df['Lemma'] = df['Lemma'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c37f1",
   "metadata": {},
   "source": [
    "# Частотный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Униграммы\n",
    "top_w = pd.DataFrame(pd.Series(' '.join(df['Lemma']).split()).value_counts())\n",
    "top_w.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ffd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если частотный анализ показывает лишние слова, не включенные в изначальный список стоп-слов, можно удалить их отдельно\n",
    "add_sw = ['...']\n",
    "df['Lemma'] = df['Lemma'].apply(lambda x: ' '.join([word for word in x.split() if word not in add_sw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593bdcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Биграммы\n",
    "import nltk\n",
    "\n",
    "tokens = ' '.join(df['Lemma']).split()\n",
    "bigrams = nltk.bigrams(tokens)\n",
    "frequence = nltk.FreqDist(bigrams)\n",
    "\n",
    "freq = pd.DataFrame(frequence.items(), columns=['word', 'frequency'])\n",
    "freq = freq.sort_values(by=['frequency'], ascending=False)\n",
    "freq['word'] = freq['word'].apply(lambda x: ' '.join(x))\n",
    "freq.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0483f",
   "metadata": {},
   "source": [
    "# Анализ тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620aaeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizerFast\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('blanchefort/rubert-base-cased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('blanchefort/rubert-base-cased-sentiment', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7768ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    predicted = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    predicted = torch.argmax(predicted, dim=1).numpy()\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['Текст'].apply(predict)\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df.groupby('sentiment').size().reset_index(), \n",
    "             x=\"sentiment\", y=0, color = 'sentiment',\n",
    "             color_discrete_sequence = ['blue', 'green', 'red'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fb1e0",
   "metadata": {},
   "source": [
    "# Тематическое моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6070df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('intfloat/multilingual-e5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model = embedding_model,\n",
    "    n_gram_range = (1, 2),\n",
    "    nr_topics = 'auto',\n",
    "    calculate_probabilities = False,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df['Lemma'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a507ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
